{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  np.reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,)\n",
      "[1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.array([1,2,3,4])\n",
    "print(a.shape)\n",
    "print(a.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,)\n",
      "[[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.array([1,2,3,4])\n",
    "print(a.shape)\n",
    "print(a.reshape(4,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### axis=0, axis=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2 3]\n",
      " [4 5 6 7]]\n",
      "[ 6 22]\n",
      "[ 4  6  8 10]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.arange(8).reshape(2,4)\n",
    "print(a)\n",
    "print(np.sum(a, axis=1))\n",
    "print(np.sum(a, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleLayer:    \n",
    "    def __init__(self, learning_rate=0.1, l1=0, l2=0):\n",
    "        self.w = None              # 가중치\n",
    "        self.b = None              # 절편\n",
    "        self.losses = []           # 훈련 손실\n",
    "        self.val_losses = []       # 검증 손실\n",
    "        self.w_history = []        # 가중치 기록\n",
    "        self.lr = learning_rate    # 학습률\n",
    "        self.l1 = l1               # L1 손실 하이퍼파라미터\n",
    "        self.l2 = l2               # L2 손실 하이퍼파라미터\n",
    "\n",
    "    def forpass(self, x):\n",
    "        z = np.dot(x, self.w) + self.b        # 선형 출력을 계산합니다.\n",
    "        return z\n",
    "\n",
    "    def backprop(self, x, err):\n",
    "        m = len(x)\n",
    "        w_grad = np.dot(x.T, err) / m         # 가중치에 대한 그래디언트를 계산합니다.\n",
    "        b_grad = np.sum(err) / m              # 절편에 대한 그래디언트를 계산합니다.\n",
    "        return w_grad, b_grad\n",
    "\n",
    "    def activation(self, z):\n",
    "        z = np.clip(z, -100, None)            # 안전한 np.exp() 계산을 위해\n",
    "        a = 1 / (1 + np.exp(-z))              # 시그모이드 계산\n",
    "        return a\n",
    "        \n",
    "    def fit(self, x, y, epochs=100, x_val=None, y_val=None):\n",
    "        y = y.reshape(-1, 1)                  # 타깃을 열 벡터로 바꿉니다.\n",
    "        y_val = y_val.reshape(-1, 1)\n",
    "        m = len(x)                            # 샘플 개수를 저장합니다.\n",
    "        self.w = np.ones((x.shape[1], 1))     # 가중치를 초기화합니다.\n",
    "        self.b = 0                            # 절편을 초기화합니다.\n",
    "        self.w_history.append(self.w.copy())  # 가중치를 기록합니다.\n",
    "        # epochs만큼 반복합니다.\n",
    "        for i in range(epochs):\n",
    "            z = self.forpass(x)               # 정방향 계산을 수행합니다.\n",
    "            a = self.activation(z)            # 활성화 함수를 적용합니다.\n",
    "            err = a - y                   # 오차를 계산합니다.\n",
    "            # 오차를 역전파하여 그래디언트를 계산합니다.\n",
    "            w_grad, b_grad = self.backprop(x, err)\n",
    "            # 그래디언트에서 페널티 항의 미분 값을 더합니다.\n",
    "            w_grad += (self.l1 * np.sign(self.w) + self.l2 * self.w) / m\n",
    "            # 가중치와 절편을 업데이트합니다.\n",
    "            self.w -= self.lr * w_grad\n",
    "            self.b -= self.lr * b_grad\n",
    "            # 가중치를 기록합니다.\n",
    "            self.w_history.append(self.w.copy())\n",
    "            # 안전한 로그 계산을 위해 클리핑합니다.\n",
    "            a = np.clip(a, 1e-10, 1-1e-10)\n",
    "            # 로그 손실과 규제 손실을 더하여 리스트에 추가합니다.\n",
    "            loss = np.sum(-(y*np.log(a) + (1-y)*np.log(1-a)))\n",
    "            self.losses.append((loss + self.reg_loss()) / m)\n",
    "            # 검증 세트에 대한 손실을 계산합니다.\n",
    "            self.update_val_loss(x_val, y_val)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        z = self.forpass(x)      # 정방향 계산을 수행합니다.\n",
    "        return z > 0             # 스텝 함수를 적용합니다.\n",
    "    \n",
    "    def score(self, x, y):\n",
    "        # 예측과 타깃 열 벡터를 비교하여 True의 비율을 반환합니다.\n",
    "        return np.mean(self.predict(x) == y.reshape(-1, 1))\n",
    "    \n",
    "    def reg_loss(self):\n",
    "        # 가중치에 규제를 적용합니다.\n",
    "        return self.l1 * np.sum(np.abs(self.w)) + self.l2 / 2 * np.sum(self.w**2)\n",
    "    \n",
    "    def update_val_loss(self, x_val, y_val):\n",
    "        z = self.forpass(x_val)            # 정방향 계산을 수행합니다.\n",
    "        a = self.activation(z)             # 활성화 함수를 적용합니다.\n",
    "        a = np.clip(a, 1e-10, 1-1e-10)     # 출력 값을 클리핑합니다.\n",
    "        # 로그 손실과 규제 손실을 더하여 리스트에 추가합니다.\n",
    "        val_loss = np.sum(-(y_val*np.log(a) + (1-y_val)*np.log(1-a)))\n",
    "        self.val_losses.append((val_loss + self.reg_loss()) / len(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualLayer(SingleLayer):\n",
    "    \n",
    "    def __init__(self, units=10, learning_rate=0.1, l1=0, l2=0):\n",
    "        self.units = units         # 은닉층의 뉴런 개수\n",
    "        self.w1 = None             # 은닉층의 가중치\n",
    "        self.b1 = None             # 은닉층의 절편\n",
    "        self.w2 = None             # 출력층의 가중치\n",
    "        self.b2 = None             # 출력층의 절편\n",
    "        self.a1 = None             # 은닉층의 활성화 출력\n",
    "        self.losses = []           # 훈련 손실\n",
    "        self.val_losses = []       # 검증 손실\n",
    "        self.lr = learning_rate    # 학습률\n",
    "        self.l1 = l1               # L1 손실 하이퍼파라미터\n",
    "        self.l2 = l2               # L2 손실 하이퍼파라미터\n",
    "\n",
    "    def forpass(self, x):\n",
    "        z1 = np.dot(x, self.w1) + self.b1        # 첫 번째 층의 선형 식을 계산합니다\n",
    "        self.a1 = self.activation(z1)            # 활성화 함수를 적용합니다\n",
    "        z2 = np.dot(self.a1, self.w2) + self.b2  # 두 번째 층의 선형 식을 계산합니다.\n",
    "        return z2\n",
    "\n",
    "    def backprop(self, x, err):\n",
    "        m = len(x)       # 샘플 개수\n",
    "        # 출력층의 가중치와 절편에 대한 그래디언트를 계산합니다.\n",
    "        w2_grad = np.dot(self.a1.T, err) / m\n",
    "        b2_grad = np.sum(err, axis=0) / m\n",
    "        # 시그모이드 함수까지 그래디언트를 계산합니다.\n",
    "        err_to_hidden = np.dot(err, self.w2.T) * self.a1 * (1 - self.a1)\n",
    "        # 은닉층의 가중치와 절편에 대한 그래디언트를 계산합니다.\n",
    "        w1_grad = np.dot(x.T, err_to_hidden) / m\n",
    "        b1_grad = np.sum(err_to_hidden, axis=0) / m\n",
    "        return w1_grad, b1_grad, w2_grad, b2_grad\n",
    "\n",
    "    def init_weights(self, n_features):\n",
    "        self.w1 = np.ones((n_features, self.units))  # (특성 개수, 은닉층의 크기)\n",
    "        self.b1 = np.zeros(self.units)               # 은닉층의 크기\n",
    "        self.w2 = np.ones((self.units, 1))           # (은닉층의 크기, 1)\n",
    "        self.b2 = 0\n",
    "        \n",
    "    def fit(self, x, y, epochs=100, x_val=None, y_val=None):\n",
    "        y = y.reshape(-1, 1)          # 타깃을 열 벡터로 바꿉니다.\n",
    "        y_val = y_val.reshape(-1, 1)\n",
    "        m = len(x)                    # 샘플 개수를 저장합니다.\n",
    "        self.init_weights(x.shape[1]) # 은닉층과 출력층의 가중치를 초기화합니다.\n",
    "        # epochs만큼 반복합니다.\n",
    "        for i in range(epochs):\n",
    "            a = self.training(x, y, m)\n",
    "            # 안전한 로그 계산을 위해 클리핑합니다.\n",
    "            a = np.clip(a, 1e-10, 1-1e-10)\n",
    "            # 로그 손실과 규제 손실을 더하여 리스트에 추가합니다.\n",
    "            loss = np.sum(-(y*np.log(a) + (1-y)*np.log(1-a)))\n",
    "            self.losses.append((loss + self.reg_loss()) / m)\n",
    "            # 검증 세트에 대한 손실을 계산합니다.\n",
    "            self.update_val_loss(x_val, y_val)\n",
    "            \n",
    "    def training(self, x, y, m):\n",
    "        z = self.forpass(x)       # 정방향 계산을 수행합니다.\n",
    "        a = self.activation(z)    # 활성화 함수를 적용합니다.\n",
    "        err = (a - y)            # 오차를 계산합니다.\n",
    "        # 오차를 역전파하여 그래디언트를 계산합니다.\n",
    "        w1_grad, b1_grad, w2_grad, b2_grad = self.backprop(x, err)\n",
    "        # 그래디언트에서 페널티 항의 미분 값을 뺍니다\n",
    "        w1_grad += (self.l1 * np.sign(self.w1) + self.l2 * self.w1) / m\n",
    "        w2_grad += (self.l1 * np.sign(self.w2) + self.l2 * self.w2) / m\n",
    "        # 은닉층의 가중치와 절편을 업데이트합니다.\n",
    "        self.w1 -= self.lr * w1_grad\n",
    "        self.b1 -= self.lr * b1_grad\n",
    "        # 출력층의 가중치와 절편을 업데이트합니다.\n",
    "        self.w2 -= self.lr * w2_grad\n",
    "        self.b2 -= self.lr * b2_grad\n",
    "        return a\n",
    "    \n",
    "    def reg_loss(self):\n",
    "        # 은닉층과 출력층의 가중치에 규제를 적용합니다.\n",
    "        return self.l1 * (np.sum(np.abs(self.w1)) + np.sum(np.abs(self.w2))) + \\\n",
    "               self.l2 / 2 * (np.sum(self.w1**2) + np.sum(self.w2**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomInitNetwork(DualLayer):\n",
    "    \n",
    "    def init_weights(self, n_features):\n",
    "        np.random.seed(42)\n",
    "        self.w1 = np.random.normal(0, 1, \n",
    "                                   (n_features, self.units))  # (특성 개수, 은닉층의 크기)\n",
    "        self.b1 = np.zeros(self.units)                        # 은닉층의 크기\n",
    "        self.w2 = np.random.normal(0, 1, \n",
    "                                   (self.units, 1))           # (은닉층의 크기, 1)\n",
    "        self.b2 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinibatchNetwork(RandomInitNetwork):\n",
    "    \n",
    "    def __init__(self, units=10, batch_size=32, learning_rate=0.1, l1=0, l2=0):\n",
    "        super().__init__(units, learning_rate, l1, l2)\n",
    "        self.batch_size = batch_size     # 배치 크기\n",
    "        \n",
    "    def fit(self, x, y, epochs=100, x_val=None, y_val=None):\n",
    "        y_val = y_val.reshape(-1, 1)     # 타깃을 열 벡터로 바꿉니다.\n",
    "        self.init_weights(x.shape[1])    # 은닉층과 출력층의 가중치를 초기화합니다.\n",
    "        np.random.seed(42)\n",
    "        # epochs만큼 반복합니다.\n",
    "        for i in range(epochs):\n",
    "            loss = 0\n",
    "            # 제너레이터 함수에서 반환한 미니배치를 순환합니다.\n",
    "            for x_batch, y_batch in self.gen_batch(x, y):\n",
    "                y_batch = y_batch.reshape(-1, 1) # 타깃을 열 벡터로 바꿉니다.\n",
    "                m = len(x_batch)                 # 샘플 개수를 저장합니다.\n",
    "                a = self.training(x_batch, y_batch, m)\n",
    "                # 안전한 로그 계산을 위해 클리핑합니다.\n",
    "                a = np.clip(a, 1e-10, 1-1e-10)\n",
    "                # 로그 손실과 규제 손실을 더하여 리스트에 추가합니다.\n",
    "                loss += np.sum(-(y_batch*np.log(a) + (1-y_batch)*np.log(1-a)))\n",
    "            self.losses.append((loss + self.reg_loss()) / len(x))\n",
    "            # 검증 세트에 대한 손실을 계산합니다.\n",
    "            self.update_val_loss(x_val, y_val)\n",
    "\n",
    "    # 미니배치 제너레이터 함수\n",
    "    def gen_batch(self, x, y):\n",
    "        length = len(x)\n",
    "        bins = length // self.batch_size # 미니배치 횟수\n",
    "        if length % self.batch_size:\n",
    "            bins += 1                    # 나누어 떨어지지 않을 때\n",
    "        indexes = np.random.permutation(np.arange(len(x))) # 인덱스를 섞습니다.\n",
    "        x = x[indexes]\n",
    "        y = y[indexes]\n",
    "        for i in range(bins):\n",
    "            start = self.batch_size * i\n",
    "            end = self.batch_size * (i + 1)\n",
    "            yield x[start:end], y[start:end]   # batch_size만큼 슬라이싱하여 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "5\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "a = np.array([4,3,5,6,2,7,1])\n",
    "y = np.array([0,0,0,0,0,0,1])\n",
    "print(np.max(a))\n",
    "print(np.argmax(a))\n",
    "print(np.argmax(a) == np.argmax(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 2]\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[4,3,5,6,2,7,1],\n",
    "              [3,4,7,5,6,2,1]])\n",
    "y = np.array([[0,0,0,0,0,1,0],\n",
    "              [0,0,0,1,0,0,0]])\n",
    "print(np.argmax(a, axis=1))\n",
    "print( np.mean(np.argmax(a, axis=1) == np.argmax(y, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassNetwork(MinibatchNetwork):\n",
    "    def __init__(self, units=10, batch_size=32, learning_rate=0.1, l1=0, l2=0):\n",
    "        self.units = units         # 은닉층의 뉴런 개수\n",
    "        self.batch_size = batch_size     # 배치 크기\n",
    "        self.w1 = None             # 은닉층의 가중치\n",
    "        self.b1 = None             # 은닉층의 절편\n",
    "        self.w2 = None             # 출력층의 가중치\n",
    "        self.b2 = None             # 출력층의 절편\n",
    "        self.a1 = None             # 은닉층의 활성화 출력\n",
    "        self.losses = []           # 훈련 손실\n",
    "        self.val_losses = []       # 검증 손실\n",
    "        self.lr = learning_rate    # 학습률\n",
    "        self.l1 = l1               # L1 손실 하이퍼파라미터\n",
    "        self.l2 = l2               # L2 손실 하이퍼파라미터\n",
    "\n",
    "    def forpass(self, x):\n",
    "        z1 = np.dot(x, self.w1) + self.b1        # 첫 번째 층의 선형 식을 계산합니다\n",
    "        self.a1 = self.sigmoid(z1)               # 활성화 함수를 적용합니다\n",
    "        z2 = np.dot(self.a1, self.w2) + self.b2  # 두 번째 층의 선형 식을 계산합니다.\n",
    "        return z2\n",
    "\n",
    "    def backprop(self, x, err):\n",
    "        m = len(x)       # 샘플 개수\n",
    "        # 출력층의 가중치와 절편에 대한 그래디언트를 계산합니다.\n",
    "        w2_grad = np.dot(self.a1.T, err) / m\n",
    "        b2_grad = np.sum(err) / m\n",
    "        # 시그모이드 함수까지 그래디언트를 계산합니다.\n",
    "        err_to_hidden = np.dot(err, self.w2.T) * self.a1 * (1 - self.a1)\n",
    "        # 은닉층의 가중치와 절편에 대한 그래디언트를 계산합니다.\n",
    "        w1_grad = np.dot(x.T, err_to_hidden) / m\n",
    "        b1_grad = np.sum(err_to_hidden, axis=0) / m\n",
    "        return w1_grad, b1_grad, w2_grad, b2_grad\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        z = np.clip(z, -100, None)            # 안전한 np.exp() 계산을 위해\n",
    "        a = 1 / (1 + np.exp(-z))              # 시그모이드 계산\n",
    "        return a\n",
    "    \n",
    "    def softmax(self, z):\n",
    "        # 소프트맥스 함수\n",
    "        z = np.clip(z, -100, None)            # 안전한 np.exp() 계산을 위해\n",
    "        exp_z = np.exp(z)\n",
    "        return exp_z / np.sum(exp_z, axis=1).reshape(-1, 1)\n",
    " \n",
    "    def init_weights(self, n_features, n_classes):\n",
    "        self.w1 = np.random.normal(0, 1, \n",
    "                                   (n_features, self.units))  # (특성 개수, 은닉층의 크기)\n",
    "        self.b1 = np.zeros(self.units)                        # 은닉층의 크기\n",
    "        self.w2 = np.random.normal(0, 1, \n",
    "                                   (self.units, n_classes))   # (은닉층의 크기, 클래스 개수)\n",
    "        self.b2 = np.zeros(n_classes)\n",
    "        \n",
    "    def fit(self, x, y, epochs=100, x_val=None, y_val=None):\n",
    "        np.random.seed(42)\n",
    "        self.init_weights(x.shape[1], y.shape[1])    # 은닉층과 출력층의 가중치를 초기화합니다.\n",
    "        # epochs만큼 반복합니다.\n",
    "        for i in range(epochs):\n",
    "            loss = 0\n",
    "            print('.', end='')\n",
    "            # 제너레이터 함수에서 반환한 미니배치를 순환합니다.\n",
    "            for x_batch, y_batch in self.gen_batch(x, y):\n",
    "                a = self.training(x_batch, y_batch)\n",
    "                # 안전한 로그 계산을 위해 클리핑합니다.\n",
    "                a = np.clip(a, 1e-10, 1-1e-10)\n",
    "                # 로그 손실과 규제 손실을 더하여 리스트에 추가합니다.\n",
    "                loss += np.sum(-y_batch*np.log(a))\n",
    "            self.losses.append((loss + self.reg_loss()) / len(x))\n",
    "            # 검증 세트에 대한 손실을 계산합니다.\n",
    "            self.update_val_loss(x_val, y_val)\n",
    "\n",
    "    # 미니배치 제너레이터 함수\n",
    "    def gen_batch(self, x, y):\n",
    "        length = len(x)\n",
    "        bins = length // self.batch_size # 미니배치 횟수\n",
    "        if length % self.batch_size:\n",
    "            bins += 1                    # 나누어 떨어지지 않을 때\n",
    "        indexes = np.random.permutation(np.arange(len(x))) # 인덱스를 섞습니다.\n",
    "        x = x[indexes]\n",
    "        y = y[indexes]\n",
    "        for i in range(bins):\n",
    "            start = self.batch_size * i\n",
    "            end = self.batch_size * (i + 1)\n",
    "            yield x[start:end], y[start:end]   # batch_size만큼 슬라이싱하여 반환합니다.\n",
    "            \n",
    "    def training(self, x, y):\n",
    "        m = len(x)                # 샘플 개수를 저장합니다.\n",
    "        z = self.forpass(x)       # 정방향 계산을 수행합니다.\n",
    "        a = self.softmax(z)       # 활성화 함수를 적용합니다.\n",
    "        err = (a - y)            # 오차를 계산합니다.\n",
    "        # 오차를 역전파하여 그래디언트를 계산합니다.\n",
    "        w1_grad, b1_grad, w2_grad, b2_grad = self.backprop(x, err)\n",
    "        # 그래디언트에서 페널티 항의 미분 값을 뺍니다\n",
    "        w1_grad += (self.l1 * np.sign(self.w1) + self.l2 * self.w1) / m\n",
    "        w2_grad += (self.l1 * np.sign(self.w2) + self.l2 * self.w2) / m\n",
    "        # 은닉층의 가중치와 절편을 업데이트합니다.\n",
    "        self.w1 -= self.lr * w1_grad\n",
    "        self.b1 -= self.lr * b1_grad\n",
    "        # 출력층의 가중치와 절편을 업데이트합니다.\n",
    "        self.w2 -= self.lr * w2_grad\n",
    "        self.b2 -= self.lr * b2_grad\n",
    "        return a\n",
    "   \n",
    "    def predict(self, x):\n",
    "        z = self.forpass(x)          # 정방향 계산을 수행합니다.\n",
    "        return np.argmax(z, axis=1)  # 가장 큰 값의 인덱스를 반환합니다.\n",
    "    \n",
    "    def score(self, x, y):\n",
    "        # 예측과 타깃 열 벡터를 비교하여 True의 비율을 반환합니다.\n",
    "        return np.mean(self.predict(x) == np.argmax(y, axis=1))\n",
    "\n",
    "    def reg_loss(self):\n",
    "        # 은닉층과 출력층의 가중치에 규제를 적용합니다.\n",
    "        return self.l1 * (np.sum(np.abs(self.w1)) + np.sum(np.abs(self.w2))) + \\\n",
    "               self.l2 / 2 * (np.sum(self.w1**2) + np.sum(self.w2**2))\n",
    "\n",
    "    def update_val_loss(self, x_val, y_val):\n",
    "        z = self.forpass(x_val)            # 정방향 계산을 수행합니다.\n",
    "        a = self.softmax(z)                # 활성화 함수를 적용합니다.\n",
    "        a = np.clip(a, 1e-10, 1-1e-10)     # 출력 값을 클리핑합니다.\n",
    "        # 크로스 엔트로피 손실과 규제 손실을 더하여 리스트에 추가합니다.\n",
    "        val_loss = np.sum(-y_val*np.log(a))\n",
    "        self.val_losses.append((val_loss + self.reg_loss()) / len(y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/home/linux/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.6.2-cp36-cp36m-manylinux2010_x86_64.whl (458.3 MB)\n",
      "     |████████████████████████████████| 458.3 MB 13.0 MB/s            \n",
      "\u001b[?25hCollecting flatbuffers~=1.12.0\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting opt-einsum~=3.3.0\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "     |████████████████████████████████| 65 kB 43.8 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.19.5)\n",
      "Collecting termcolor~=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting keras-preprocessing~=1.1.2\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "     |████████████████████████████████| 42 kB 40.5 MB/s            \n",
      "\u001b[?25hCollecting protobuf>=3.9.2\n",
      "  Downloading protobuf-3.19.1-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "     |████████████████████████████████| 1.1 MB 20.8 MB/s            \n",
      "\u001b[?25hCollecting keras<2.7,>=2.6.0\n",
      "  Downloading keras-2.6.0-py2.py3-none-any.whl (1.3 MB)\n",
      "     |████████████████████████████████| 1.3 MB 28.6 MB/s            \n",
      "\u001b[?25hCollecting astunparse~=1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting tensorboard<2.7,>=2.6.0\n",
      "  Downloading tensorboard-2.6.0-py3-none-any.whl (5.6 MB)\n",
      "     |████████████████████████████████| 5.6 MB 11.7 MB/s            \n",
      "\u001b[?25hCollecting tensorflow-estimator<2.7,>=2.6.0\n",
      "  Downloading tensorflow_estimator-2.6.0-py2.py3-none-any.whl (462 kB)\n",
      "     |████████████████████████████████| 462 kB 31.3 MB/s            \n",
      "\u001b[?25hCollecting gast==0.4.0\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting wheel~=0.35\n",
      "  Downloading wheel-0.37.0-py2.py3-none-any.whl (35 kB)\n",
      "Collecting grpcio<2.0,>=1.37.0\n",
      "  Downloading grpcio-1.42.0-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
      "     |████████████████████████████████| 4.0 MB 47.7 MB/s            \n",
      "\u001b[?25hCollecting typing-extensions~=3.7.4\n",
      "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Collecting clang~=5.0\n",
      "  Downloading clang-5.0.tar.gz (30 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting wrapt~=1.12.1\n",
      "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting h5py~=3.1.0\n",
      "  Downloading h5py-3.1.0-cp36-cp36m-manylinux1_x86_64.whl (4.0 MB)\n",
      "     |████████████████████████████████| 4.0 MB 23.9 MB/s            \n",
      "\u001b[?25hCollecting six~=1.15.0\n",
      "  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting google-pasta~=0.2\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "     |████████████████████████████████| 57 kB 33.2 MB/s            \n",
      "\u001b[?25hCollecting absl-py~=0.10\n",
      "  Downloading absl_py-0.15.0-py3-none-any.whl (132 kB)\n",
      "     |████████████████████████████████| 132 kB 21.3 MB/s            \n",
      "\u001b[?25hCollecting cached-property\n",
      "  Downloading cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "     |████████████████████████████████| 781 kB 26.2 MB/s            \n",
      "\u001b[?25hCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.6-py3-none-any.whl (97 kB)\n",
      "     |████████████████████████████████| 97 kB 29.8 MB/s            \n",
      "\u001b[?25hCollecting requests<3,>=2.21.0\n",
      "  Downloading requests-2.26.0-py2.py3-none-any.whl (62 kB)\n",
      "     |████████████████████████████████| 62 kB 28.1 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /home/linux/.local/lib/python3.6/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (59.5.0)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "     |████████████████████████████████| 4.9 MB 28.7 MB/s            \n",
      "\u001b[?25hCollecting werkzeug>=0.11.15\n",
      "  Downloading Werkzeug-2.0.2-py3-none-any.whl (288 kB)\n",
      "     |████████████████████████████████| 288 kB 43.6 MB/s            \n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n",
      "     |████████████████████████████████| 152 kB 26.2 MB/s            \n",
      "\u001b[?25hCollecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "     |████████████████████████████████| 155 kB 19.7 MB/s            \n",
      "\u001b[?25hCollecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.8-py3-none-any.whl (39 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/linux/.local/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard<2.7,>=2.6.0->tensorflow) (4.8.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow) (2018.1.18)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow) (1.22)\n",
      "Collecting charset-normalizer~=2.0.0\n",
      "  Downloading charset_normalizer-2.0.9-py3-none-any.whl (39 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow) (2.6)\n",
      "Collecting dataclasses\n",
      "  Downloading dataclasses-0.8-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/linux/.local/lib/python3.6/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.7,>=2.6.0->tensorflow) (3.6.0)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "     |████████████████████████████████| 77 kB 37.2 MB/s            \n",
      "\u001b[?25hCollecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.1-py2.py3-none-any.whl (146 kB)\n",
      "     |████████████████████████████████| 146 kB 28.2 MB/s            \n",
      "\u001b[?25hBuilding wheels for collected packages: clang, termcolor, wrapt\n",
      "  Building wheel for clang (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for clang: filename=clang-5.0-py3-none-any.whl size=32130 sha256=cc9bdc11a84417048c12597bf0bc6ee3746236b1f78e214deb31da1f8db43cca\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-5ph695n_/wheels/22/4c/94/0583f60c9c5b6024ed64f290cb2d43b06bb4f75577dc3c93a7\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=5714 sha256=38f751f88c24348f01765239f276ef33a9d98564d05bf3c376dc120dffcba1ea\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-5ph695n_/wheels/93/2a/eb/e58dbcbc963549ee4f065ff80a59f274cc7210b6eab962acdc\n",
      "  Building wheel for wrapt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wrapt: filename=wrapt-1.12.1-cp36-cp36m-linux_x86_64.whl size=69407 sha256=cfee4d6b075188c3da5b7af5cde21e8d80c82b2d9b33c5cb515d404536ae6bed\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-5ph695n_/wheels/32/42/7f/23cae9ff6ef66798d00dc5d659088e57dbba01566f6c60db63\n",
      "Successfully built clang termcolor wrapt\n",
      "Installing collected packages: pyasn1, charset-normalizer, typing-extensions, six, rsa, requests, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, google-auth, dataclasses, wheel, werkzeug, tensorboard-plugin-wit, tensorboard-data-server, protobuf, markdown, grpcio, google-auth-oauthlib, cached-property, absl-py, wrapt, termcolor, tensorflow-estimator, tensorboard, opt-einsum, keras-preprocessing, keras, h5py, google-pasta, gast, flatbuffers, clang, astunparse, tensorflow\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 4.0.1\n",
      "    Uninstalling typing-extensions-4.0.1:\n",
      "      Successfully uninstalled typing-extensions-4.0.1\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Successfully uninstalled six-1.16.0\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.18.4\n",
      "    Uninstalling requests-2.18.4:\n",
      "      Successfully uninstalled requests-2.18.4\n",
      "  Attempting uninstall: wheel\n",
      "    Found existing installation: wheel 0.30.0\n",
      "    Uninstalling wheel-0.30.0:\n",
      "      Successfully uninstalled wheel-0.30.0\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.0.0\n",
      "    Uninstalling protobuf-3.0.0:\n",
      "      Successfully uninstalled protobuf-3.0.0\n",
      "Successfully installed absl-py-0.15.0 astunparse-1.6.3 cached-property-1.5.2 cachetools-4.2.4 charset-normalizer-2.0.9 clang-5.0 dataclasses-0.8 flatbuffers-1.12 gast-0.4.0 google-auth-1.35.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.42.0 h5py-3.1.0 keras-2.6.0 keras-preprocessing-1.1.2 markdown-3.3.6 oauthlib-3.1.1 opt-einsum-3.3.0 protobuf-3.19.1 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-2.26.0 requests-oauthlib-1.3.0 rsa-4.8 six-1.15.0 tensorboard-2.6.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 tensorflow-2.6.2 tensorflow-estimator-2.6.0 termcolor-1.1.0 typing-extensions-4.0.1 werkzeug-2.0.2 wheel-0.37.0 wrapt-1.12.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 0s 0us/step\n",
      "40960/29515 [=========================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - 1s 0us/step\n",
      "26435584/26421880 [==============================] - 1s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "16384/5148 [===============================================================================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - 0s 0us/step\n",
      "4431872/4422102 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "(x_train_all, y_train_all), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,)\n",
      "(10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_all.shape, y_train_all.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_all.shape)\n",
    "print(x_train_all[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAR10lEQVR4nO3db2yVdZYH8O+xgNqCBaxA+RPBESOTjVvWikbRjI4Q9IUwanB4scGo24kZk5lkTNa4L8bEFxLdmcm+IJN01AyzzjqZZCBi/DcMmcTdFEcqYdtKd0ZACK2lBUFoS6EUzr7og+lgn3Pqfe69z5Xz/SSk7T393fvrvf1yb+95fs9PVBVEdOm7LO8JEFF5MOxEQTDsREEw7ERBMOxEQUwq542JCN/6JyoxVZXxLs/0zC4iq0TkryKyV0SeyXJdRFRaUmifXUSqAPwNwAoAXQB2AlinqnuMMXxmJyqxUjyzLwOwV1X3q+owgN8BWJ3h+oiohLKEfR6AQ2O+7kou+zsi0iQirSLSmuG2iCijkr9Bp6rNAJoBvownylOWZ/ZuAAvGfD0/uYyIKlCWsO8EsFhEFonIFADfB7C1ONMiomIr+GW8qo6IyFMA3gNQBeBVVf24aDMjoqIquPVW0I3xb3aikivJQTVE9M3BsBMFwbATBcGwEwXBsBMFwbATBcGwEwXBsBMFwbATBcGwEwXBsBMFwbATBcGwEwVR1lNJU/mJjLsA6ktZVz1OmzbNrC9fvjy19s4772S6be9nq6qqSq2NjIxkuu2svLlbCn3M+MxOFATDThQEw04UBMNOFATDThQEw04UBMNOFAT77Je4yy6z/z8/d+6cWb/++uvN+hNPPGHWh4aGUmuDg4Pm2NOnT5v1Dz/80Kxn6aV7fXDvfvXGZ5mbdfyA9XjymZ0oCIadKAiGnSgIhp0oCIadKAiGnSgIhp0oCPbZL3FWTxbw++z33HOPWb/33nvNeldXV2rt8ssvN8dWV1eb9RUrVpj1l19+ObXW29trjvXWjHv3m2fq1KmptfPnz5tjT506VdBtZgq7iBwA0A/gHIARVW3Mcn1EVDrFeGa/W1WPFuF6iKiE+Dc7URBZw64A/igiH4lI03jfICJNItIqIq0Zb4uIMsj6Mn65qnaLyCwA20Tk/1T1/bHfoKrNAJoBQESynd2QiAqW6ZldVbuTj30AtgBYVoxJEVHxFRx2EakRkWkXPgewEkBHsSZGRMWV5WX8bABbknW7kwD8l6q+W5RZUdEMDw9nGn/LLbeY9YULF5p1q8/vrQl/7733zPrSpUvN+osvvphaa22130Jqb283652dnWZ92TL7Ra51v7a0tJhjd+zYkVobGBhIrRUcdlXdD+AfCx1PROXF1htREAw7URAMO1EQDDtREAw7URCSdcver3VjPIKuJKzTFnuPr7dM1GpfAcD06dPN+tmzZ1Nr3lJOz86dO8363r17U2tZW5L19fVm3fq5AXvuDz/8sDl248aNqbXW1lacPHly3F8IPrMTBcGwEwXBsBMFwbATBcGwEwXBsBMFwbATBcE+ewXwtvfNwnt8P/jgA7PuLWH1WD+bt21x1l64teWz1+PftWuXWbd6+ID/s61atSq1dt1115lj582bZ9ZVlX12osgYdqIgGHaiIBh2oiAYdqIgGHaiIBh2oiC4ZXMFKOexDhc7fvy4WffWbQ8NDZl1a1vmSZPsXz9rW2PA7qMDwJVXXpla8/rsd955p1m//fbbzbp3muxZs2al1t59tzRnZOczO1EQDDtREAw7URAMO1EQDDtREAw7URAMO1EQ7LMHV11dbda9frFXP3XqVGrtxIkT5tjPP//crHtr7a3jF7xzCHg/l3e/nTt3zqxbff4FCxaYYwvlPrOLyKsi0iciHWMumyki20Tkk+TjjJLMjoiKZiIv438N4OLTajwDYLuqLgawPfmaiCqYG3ZVfR/AsYsuXg1gU/L5JgBrijstIiq2Qv9mn62qPcnnhwHMTvtGEWkC0FTg7RBRkWR+g05V1TqRpKo2A2gGeMJJojwV2nrrFZF6AEg+9hVvSkRUCoWGfSuA9cnn6wG8UZzpEFGpuC/jReR1AN8BUCciXQB+CmADgN+LyOMADgJYW8pJXuqy9nytnq63Jnzu3Llm/cyZM5nq1np277zwVo8e8PeGt/r0Xp98ypQpZr2/v9+s19bWmvW2trbUmveYNTY2ptb27NmTWnPDrqrrUkrf9cYSUeXg4bJEQTDsREEw7ERBMOxEQTDsREFwiWsF8E4lXVVVZdat1tsjjzxijp0zZ45ZP3LkiFm3TtcM2Es5a2pqzLHeUk+vdWe1/c6ePWuO9U5z7f3cV199tVnfuHFjaq2hocEca83NauPymZ0oCIadKAiGnSgIhp0oCIadKAiGnSgIhp0oCCnndsE8U834vJ7uyMhIwdd96623mvW33nrLrHtbMmc5BmDatGnmWG9LZu9U05MnTy6oBvjHAHhbXXusn+2ll14yx7722mtmXVXHbbbzmZ0oCIadKAiGnSgIhp0oCIadKAiGnSgIhp0oiG/UenZrra7X7/VOx+ydztla/2yt2Z6ILH10z9tvv23WBwcHzbrXZ/dOuWwdx+Gtlfce0yuuuMKse2vWs4z1HnNv7jfddFNqzdvKulB8ZicKgmEnCoJhJwqCYScKgmEnCoJhJwqCYScKoqL67FnWRpeyV11qd911l1l/6KGHzPodd9yRWvO2PfbWhHt9dG8tvvWYeXPzfh+s88IDdh/eO4+DNzePd78NDAyk1h588EFz7JtvvlnQnNxndhF5VUT6RKRjzGXPiUi3iOxO/t1f0K0TUdlM5GX8rwGsGufyX6hqQ/LPPkyLiHLnhl1V3wdwrAxzIaISyvIG3VMi0pa8zJ+R9k0i0iQirSLSmuG2iCijQsP+SwDfAtAAoAfAz9K+UVWbVbVRVRsLvC0iKoKCwq6qvap6TlXPA/gVgGXFnRYRFVtBYReR+jFffg9AR9r3ElFlcM8bLyKvA/gOgDoAvQB+mnzdAEABHADwA1XtcW8sx/PGz5w506zPnTvXrC9evLjgsV7f9IYbbjDrZ86cMevWWn1vXba3z/hnn31m1r3zr1v9Zm8Pc2//9erqarPe0tKSWps6dao51jv2wVvP7q1Jt+633t5ec+ySJUvMetp5492DalR13TgXv+KNI6LKwsNliYJg2ImCYNiJgmDYiYJg2ImCqKgtm2+77TZz/PPPP59au+aaa8yx06dPN+vWUkzAXm75xRdfmGO95bdeC8lrQVmnwfZOBd3Z2WnW165da9ZbW+2joK1tmWfMSD3KGgCwcOFCs+7Zv39/as3bLrq/v9+se0tgvZam1fq76qqrzLHe7wu3bCYKjmEnCoJhJwqCYScKgmEnCoJhJwqCYScKoux9dqtfvWPHDnN8fX19as3rk3v1LKcO9k557PW6s6qtrU2t1dXVmWMfffRRs75y5Uqz/uSTT5p1a4ns6dOnzbGffvqpWbf66IC9LDnr8lpvaa/Xx7fGe8tnr732WrPOPjtRcAw7URAMO1EQDDtREAw7URAMO1EQDDtREGXts9fV1ekDDzyQWt+wYYM5ft++fak179TAXt3b/tfi9VytPjgAHDp0yKx7p3O21vJbp5kGgDlz5pj1NWvWmHVrW2TAXpPuPSY333xzprr1s3t9dO9+87Zk9ljnIPB+n6zzPhw+fBjDw8PssxNFxrATBcGwEwXBsBMFwbATBcGwEwXBsBMF4e7iWkwjIyPo6+tLrXv9ZmuNsLetsXfdXs/X6qt65/k+duyYWT948KBZ9+ZmrZf31ox757TfsmWLWW9vbzfrVp/d20bb64V75+u3tqv2fm5vTbnXC/fGW312r4dvbfFt3SfuM7uILBCRP4vIHhH5WER+lFw+U0S2icgnyUf7jP9ElKuJvIwfAfATVf02gNsA/FBEvg3gGQDbVXUxgO3J10RUodywq2qPqu5KPu8H0AlgHoDVADYl37YJwJoSzZGIiuBrvUEnIgsBLAXwFwCzVbUnKR0GMDtlTJOItIpIq/c3GBGVzoTDLiJTAfwBwI9V9eTYmo6uphl3RY2qNqtqo6o2Zl08QESFm1DYRWQyRoP+W1XdnFzcKyL1Sb0eQPrb7ESUO7f1JqM9glcAdKrqz8eUtgJYD2BD8vEN77qGh4fR3d2dWveW23Z1daXWampqzLHeKZW9Ns7Ro0dTa0eOHDHHTppk383e8lqvzWMtM/VOaewt5bR+bgBYsmSJWR8cHEytee3Q48ePm3XvfrPmbrXlAL815433tmy2lhafOHHCHNvQ0JBa6+joSK1NpM9+B4B/BtAuIruTy57FaMh/LyKPAzgIwN7Im4hy5YZdVf8HQNoRAN8t7nSIqFR4uCxREAw7URAMO1EQDDtREAw7URBlXeI6NDSE3bt3p9Y3b96cWgOAxx57LLXmnW7Z297XWwpqLTP1+uBez9U7stDbEtpa3uttVe0d2+BtZd3T02PWrev35uYdn5DlMcu6fDbL8lrA7uMvWrTIHNvb21vQ7fKZnSgIhp0oCIadKAiGnSgIhp0oCIadKAiGnSiIsm7ZLCKZbuy+++5LrT399NPm2FmzZpl1b9221Vf1+sVen9zrs3v9Zuv6rVMWA36f3TuGwKtbP5s31pu7xxpv9aonwnvMvFNJW+vZ29razLFr19qryVWVWzYTRcawEwXBsBMFwbATBcGwEwXBsBMFwbATBVH2Prt1nnKvN5nF3XffbdZfeOEFs2716Wtra82x3rnZvT6812f3+vwWawttwO/DW/sAAPZjOjAwYI717hePNXdvvbm3jt97TLdt22bWOzs7U2stLS3mWA/77ETBMexEQTDsREEw7ERBMOxEQTDsREEw7ERBuH12EVkA4DcAZgNQAM2q+h8i8hyAfwFwYXPyZ1X1bee6ytfUL6Mbb7zRrGfdG37+/Plm/cCBA6k1r5+8b98+s07fPGl99olsEjEC4CequktEpgH4SEQuHDHwC1X992JNkohKZyL7s/cA6Ek+7xeRTgDzSj0xIiqur/U3u4gsBLAUwF+Si54SkTYReVVEZqSMaRKRVhFpzTZVIspiwmEXkakA/gDgx6p6EsAvAXwLQANGn/l/Nt44VW1W1UZVbcw+XSIq1ITCLiKTMRr036rqZgBQ1V5VPaeq5wH8CsCy0k2TiLJywy6jp+h8BUCnqv58zOX1Y77tewA6ij89IiqWibTelgP4bwDtAC6sV3wWwDqMvoRXAAcA/CB5M8+6rkuy9UZUSdJab9+o88YTkY/r2YmCY9iJgmDYiYJg2ImCYNiJgmDYiYJg2ImCYNiJgmDYiYJg2ImCYNiJgmDYiYJg2ImCYNiJgpjI2WWL6SiAg2O+rksuq0SVOrdKnRfAuRWqmHO7Nq1Q1vXsX7lxkdZKPTddpc6tUucFcG6FKtfc+DKeKAiGnSiIvMPenPPtWyp1bpU6L4BzK1RZ5pbr3+xEVD55P7MTUZkw7ERB5BJ2EVklIn8Vkb0i8kwec0gjIgdEpF1Edue9P12yh16fiHSMuWymiGwTkU+Sj+PusZfT3J4Tke7kvtstIvfnNLcFIvJnEdkjIh+LyI+Sy3O974x5leV+K/vf7CJSBeBvAFYA6AKwE8A6Vd1T1omkEJEDABpVNfcDMETkLgADAH6jqv+QXPYigGOquiH5j3KGqv5rhcztOQADeW/jnexWVD92m3EAawA8ihzvO2Nea1GG+y2PZ/ZlAPaq6n5VHQbwOwCrc5hHxVPV9wEcu+ji1QA2JZ9vwugvS9mlzK0iqGqPqu5KPu8HcGGb8VzvO2NeZZFH2OcBODTm6y5U1n7vCuCPIvKRiDTlPZlxzB6zzdZhALPznMw43G28y+mibcYr5r4rZPvzrPgG3VctV9V/AnAfgB8mL1crko7+DVZJvdMJbeNdLuNsM/6lPO+7Qrc/zyqPsHcDWDDm6/nJZRVBVbuTj30AtqDytqLuvbCDbvKxL+f5fKmStvEeb5txVMB9l+f253mEfSeAxSKySESmAPg+gK05zOMrRKQmeeMEIlIDYCUqbyvqrQDWJ5+vB/BGjnP5O5WyjXfaNuPI+b7LfftzVS37PwD3Y/Qd+X0A/i2POaTM6zoA/5v8+zjvuQF4HaMv685i9L2NxwFcDWA7gE8A/AnAzAqa239idGvvNowGqz6nuS3H6Ev0NgC7k3/3533fGfMqy/3Gw2WJguAbdERBMOxEQTDsREEw7ERBMOxEQTDsREEw7ERB/D/+XzeWfiVg0AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train_all[0], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9 0 0 3 0 2 7 2 5 5 0 9 5 5 7 9 1 0 6 4 3 1 4 8 4 3 0 2 4 4 5 3 6 6 0 8 5\n",
      " 2 1 6 6 7 9 5 9 2 7 3 0 3 3 3 7 2 2 6 6 8 3 3 5 0 5 5 0 2 0 0 4 1 3 1 6 3\n",
      " 1 4 4 6 1 9 1 3 5 7 9 7 1 7 9 9 9 3 2 9 3 6 4 1 1 8]\n"
     ]
    }
   ],
   "source": [
    "print(y_train_all[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "드레스\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class_names = ['티셔츠/윗도리', '바지', '스웨터', '드레스', '코트', \n",
    "               '샌들', '셔츠', '스니커즈', '가방', '앵클부츠']\n",
    "index = 3\n",
    "print(class_names[y_train_all[index]])\n",
    "# plt.title(class_names[y_train_all[index]])\n",
    "plt.imshow(x_train_all[index], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### np.bincount 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n",
      "[6 4]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([0,1,1,0,0,0,0,1,1,0])\n",
    "print(len(a))\n",
    "print(a.shape[0])\n",
    "print(np.bincount(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "14\n",
      "[2 1 1 4 3 1 2]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([0,4,4,1,2,3,3,3,3,4,5,0,6,6])\n",
    "print(len(a))\n",
    "print(a.shape[0])\n",
    "print(np.bincount(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "[6000 6000 6000 6000 6000 6000 6000 6000 6000 6000]\n"
     ]
    }
   ],
   "source": [
    "print(len(y_train_all))\n",
    "print(np.bincount(y_train_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "[1000 1000 1000 1000 1000 1000 1000 1000 1000 1000]\n"
     ]
    }
   ],
   "source": [
    "print(len(y_test))\n",
    "print(np.bincount(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train_all, y_train_all, stratify=y_train_all, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([4800, 4800, 4800, 4800, 4800, 4800, 4800, 4800, 4800, 4800])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(y_train))\n",
    "np.bincount(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(y_val))\n",
    "np.bincount(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 28, 28)\n",
      "(28, 28)\n",
      "[[  0   0   0   0   1   1   0   0   0  25  55   0   0   0   0   0   0  43\n",
      "   24   0   0   0   1   0   0   0   0   0]\n",
      " [  0   0   0   1   1   1   0  25 147 189 181  96  51  49  55  37  76 217\n",
      "  172 125  20   0   2   1   0   0   0   0]\n",
      " [  0   0   0   1   3   0  82 206 202 190 178 188 239 152  99 199 201 162\n",
      "  176 192 185  71   0   2   0   0   0   0]\n",
      " [  0   0   0   1   0   0 202 196 181 188 183 169 179 160 107 198 166 167\n",
      "  178 170 183 166   0   0   1   0   0   0]\n",
      " [  0   0   0   1   0  48 216 184 185 185 190 175 164 188 198 188 157 183\n",
      "  176 171 167 175  26   0   2   0   0   0]\n",
      " [  0   0   0   0   0  90 224 183 185 184 183 190 164 169 193 158 170 174\n",
      "  176 175 165 180  62   0   0   0   0   0]\n",
      " [  0   0   0   0   0 123 228 184 180 183 178 183 172 172 172 161 179 170\n",
      "  174 175 165 181 106   0   0   0   0   0]\n",
      " [  0   0   0   0   0 156 229 188 175 178 175 175 190 157 157 180 169 169\n",
      "  172 171 161 179 143   0   0   0   0   0]\n",
      " [  0   0   0   0   0 181 228 190 174 174 176 175 197 167 167 183 166 169\n",
      "  171 170 162 175 172   0   0   0   0   0]\n",
      " [  0   0   0   0   6 175 221 192 170 174 176 175 193 176 172 175 167 169\n",
      "  169 167 160 169 196   0   0   0   0   0]\n",
      " [  0   0   0   0  23 184 216 206 164 175 176 175 192 178 175 174 167 166\n",
      "  169 169 165 169 175   6   0   0   0   0]\n",
      " [  0   0   0   0  53 189 208 219 161 176 176 175 190 181 178 172 169 165\n",
      "  170 169 166 167 183  26   0   0   0   0]\n",
      " [  0   0   0   0  84 194 207 216 157 176 175 176 193 183 180 172 167 167\n",
      "  166 170 165 167 185  61   0   0   0   0]\n",
      " [  0   0   0   0 114 193 211 207 157 175 175 176 196 183 181 172 166 166\n",
      "  165 170 164 169 187  88   0   0   0   0]\n",
      " [  0   0   0   0 155 192 219 194 161 172 178 176 197 185 181 169 166 165\n",
      "  165 171 164 171 184 108   0   0   0   0]\n",
      " [  0   0   0   0 181 192 225 185 162 172 179 176 199 185 183 169 166 164\n",
      "  164 172 161 170 184 135   0   0   0   0]\n",
      " [  0   0   0   0 210 189 233 180 161 171 179 179 201 185 190 166 166 164\n",
      "  164 176 158 171 183 155   0   0   0   0]\n",
      " [  0   0   0   0 229 184 234 176 164 169 180 183 198 185 193 166 167 164\n",
      "  165 179 157 176 179 174   0   0   0   0]\n",
      " [  0   0   0   2 237 183 237 171 166 167 178 188 196 187 196 164 166 165\n",
      "  166 179 157 178 176 185   2   0   0   0]\n",
      " [  0   0   0  24 246 184 255 172 166 169 180 192 196 189 201 161 164 165\n",
      "  165 181 161 181 176 189  17   0   0   0]\n",
      " [  0   0   0  52 247 187 238 166 170 170 180 198 193 189 207 160 165 165\n",
      "  164 180 162 187 175 192  52   0   0   0]\n",
      " [  0   0   0  82 215 192 239 158 169 174 179 207 189 187 213 160 167 166\n",
      "  165 181 169 190 170 192  73   0   0   0]\n",
      " [  0   0   0 102 212 201 212 166 169 171 179 217 189 187 217 157 170 167\n",
      "  166 178 171 196 169 171  84   0   0   0]\n",
      " [  0   0   0 166 211 213 193 164 170 170 180 226 181 187 228 157 167 167\n",
      "  158 188 196 128 170 179 128   0   0   0]\n",
      " [  0   0   0  35  92 129 199 184 171 179 187 246 170 189 240 156 170 174\n",
      "  166 190 230 139 102 139  89   0   0   0]\n",
      " [  0   0   0   0   0   0 142 213 166 151 178 235 190 162 244 179 183 160\n",
      "  142 164 208 125   0   0   0   0   0   0]\n",
      " [  0   0   0   0   1   2   0  96 181 179 178 178 185 160 208 171 167 167\n",
      "  184 174 105   0   1   2   1   0   0   0]\n",
      " [  0   0   0   0   0   0   2   0   1  52  83  98 112 134 146 105  89  73\n",
      "   49   0   0   0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_train[0].shape)  # (48000,28,28)\n",
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  표준화(standardization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train / 255\n",
    "x_val = x_val / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.0\n",
      "0.2862528489520806\n",
      "0.3531738779381928\n"
     ]
    }
   ],
   "source": [
    "print(np.max(x_train))\n",
    "print(np.min(x_train))\n",
    "print(np.mean(x_train))\n",
    "print(np.std(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.00392157 0.00392157\n",
      "  0.         0.         0.         0.09803922 0.21568627 0.\n",
      "  0.         0.         0.         0.         0.         0.16862745\n",
      "  0.09411765 0.         0.         0.         0.00392157 0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.00392157 0.00392157 0.00392157\n",
      "  0.         0.09803922 0.57647059 0.74117647 0.70980392 0.37647059\n",
      "  0.2        0.19215686 0.21568627 0.14509804 0.29803922 0.85098039\n",
      "  0.6745098  0.49019608 0.07843137 0.         0.00784314 0.00392157\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.00392157 0.01176471 0.\n",
      "  0.32156863 0.80784314 0.79215686 0.74509804 0.69803922 0.7372549\n",
      "  0.9372549  0.59607843 0.38823529 0.78039216 0.78823529 0.63529412\n",
      "  0.69019608 0.75294118 0.7254902  0.27843137 0.         0.00784314\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.00392157 0.         0.\n",
      "  0.79215686 0.76862745 0.70980392 0.7372549  0.71764706 0.6627451\n",
      "  0.70196078 0.62745098 0.41960784 0.77647059 0.65098039 0.65490196\n",
      "  0.69803922 0.66666667 0.71764706 0.65098039 0.         0.\n",
      "  0.00392157 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.00392157 0.         0.18823529\n",
      "  0.84705882 0.72156863 0.7254902  0.7254902  0.74509804 0.68627451\n",
      "  0.64313725 0.7372549  0.77647059 0.7372549  0.61568627 0.71764706\n",
      "  0.69019608 0.67058824 0.65490196 0.68627451 0.10196078 0.\n",
      "  0.00784314 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.35294118\n",
      "  0.87843137 0.71764706 0.7254902  0.72156863 0.71764706 0.74509804\n",
      "  0.64313725 0.6627451  0.75686275 0.61960784 0.66666667 0.68235294\n",
      "  0.69019608 0.68627451 0.64705882 0.70588235 0.24313725 0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.48235294\n",
      "  0.89411765 0.72156863 0.70588235 0.71764706 0.69803922 0.71764706\n",
      "  0.6745098  0.6745098  0.6745098  0.63137255 0.70196078 0.66666667\n",
      "  0.68235294 0.68627451 0.64705882 0.70980392 0.41568627 0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.61176471\n",
      "  0.89803922 0.7372549  0.68627451 0.69803922 0.68627451 0.68627451\n",
      "  0.74509804 0.61568627 0.61568627 0.70588235 0.6627451  0.6627451\n",
      "  0.6745098  0.67058824 0.63137255 0.70196078 0.56078431 0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.70980392\n",
      "  0.89411765 0.74509804 0.68235294 0.68235294 0.69019608 0.68627451\n",
      "  0.77254902 0.65490196 0.65490196 0.71764706 0.65098039 0.6627451\n",
      "  0.67058824 0.66666667 0.63529412 0.68627451 0.6745098  0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.02352941 0.68627451\n",
      "  0.86666667 0.75294118 0.66666667 0.68235294 0.69019608 0.68627451\n",
      "  0.75686275 0.69019608 0.6745098  0.68627451 0.65490196 0.6627451\n",
      "  0.6627451  0.65490196 0.62745098 0.6627451  0.76862745 0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.09019608 0.72156863\n",
      "  0.84705882 0.80784314 0.64313725 0.68627451 0.69019608 0.68627451\n",
      "  0.75294118 0.69803922 0.68627451 0.68235294 0.65490196 0.65098039\n",
      "  0.6627451  0.6627451  0.64705882 0.6627451  0.68627451 0.02352941\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.20784314 0.74117647\n",
      "  0.81568627 0.85882353 0.63137255 0.69019608 0.69019608 0.68627451\n",
      "  0.74509804 0.70980392 0.69803922 0.6745098  0.6627451  0.64705882\n",
      "  0.66666667 0.6627451  0.65098039 0.65490196 0.71764706 0.10196078\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.32941176 0.76078431\n",
      "  0.81176471 0.84705882 0.61568627 0.69019608 0.68627451 0.69019608\n",
      "  0.75686275 0.71764706 0.70588235 0.6745098  0.65490196 0.65490196\n",
      "  0.65098039 0.66666667 0.64705882 0.65490196 0.7254902  0.23921569\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.44705882 0.75686275\n",
      "  0.82745098 0.81176471 0.61568627 0.68627451 0.68627451 0.69019608\n",
      "  0.76862745 0.71764706 0.70980392 0.6745098  0.65098039 0.65098039\n",
      "  0.64705882 0.66666667 0.64313725 0.6627451  0.73333333 0.34509804\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.60784314 0.75294118\n",
      "  0.85882353 0.76078431 0.63137255 0.6745098  0.69803922 0.69019608\n",
      "  0.77254902 0.7254902  0.70980392 0.6627451  0.65098039 0.64705882\n",
      "  0.64705882 0.67058824 0.64313725 0.67058824 0.72156863 0.42352941\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.70980392 0.75294118\n",
      "  0.88235294 0.7254902  0.63529412 0.6745098  0.70196078 0.69019608\n",
      "  0.78039216 0.7254902  0.71764706 0.6627451  0.65098039 0.64313725\n",
      "  0.64313725 0.6745098  0.63137255 0.66666667 0.72156863 0.52941176\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.82352941 0.74117647\n",
      "  0.91372549 0.70588235 0.63137255 0.67058824 0.70196078 0.70196078\n",
      "  0.78823529 0.7254902  0.74509804 0.65098039 0.65098039 0.64313725\n",
      "  0.64313725 0.69019608 0.61960784 0.67058824 0.71764706 0.60784314\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.89803922 0.72156863\n",
      "  0.91764706 0.69019608 0.64313725 0.6627451  0.70588235 0.71764706\n",
      "  0.77647059 0.7254902  0.75686275 0.65098039 0.65490196 0.64313725\n",
      "  0.64705882 0.70196078 0.61568627 0.69019608 0.70196078 0.68235294\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.00784314 0.92941176 0.71764706\n",
      "  0.92941176 0.67058824 0.65098039 0.65490196 0.69803922 0.7372549\n",
      "  0.76862745 0.73333333 0.76862745 0.64313725 0.65098039 0.64705882\n",
      "  0.65098039 0.70196078 0.61568627 0.69803922 0.69019608 0.7254902\n",
      "  0.00784314 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.09411765 0.96470588 0.72156863\n",
      "  1.         0.6745098  0.65098039 0.6627451  0.70588235 0.75294118\n",
      "  0.76862745 0.74117647 0.78823529 0.63137255 0.64313725 0.64705882\n",
      "  0.64705882 0.70980392 0.63137255 0.70980392 0.69019608 0.74117647\n",
      "  0.06666667 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.20392157 0.96862745 0.73333333\n",
      "  0.93333333 0.65098039 0.66666667 0.66666667 0.70588235 0.77647059\n",
      "  0.75686275 0.74117647 0.81176471 0.62745098 0.64705882 0.64705882\n",
      "  0.64313725 0.70588235 0.63529412 0.73333333 0.68627451 0.75294118\n",
      "  0.20392157 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.32156863 0.84313725 0.75294118\n",
      "  0.9372549  0.61960784 0.6627451  0.68235294 0.70196078 0.81176471\n",
      "  0.74117647 0.73333333 0.83529412 0.62745098 0.65490196 0.65098039\n",
      "  0.64705882 0.70980392 0.6627451  0.74509804 0.66666667 0.75294118\n",
      "  0.28627451 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.4        0.83137255 0.78823529\n",
      "  0.83137255 0.65098039 0.6627451  0.67058824 0.70196078 0.85098039\n",
      "  0.74117647 0.73333333 0.85098039 0.61568627 0.66666667 0.65490196\n",
      "  0.65098039 0.69803922 0.67058824 0.76862745 0.6627451  0.67058824\n",
      "  0.32941176 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.65098039 0.82745098 0.83529412\n",
      "  0.75686275 0.64313725 0.66666667 0.66666667 0.70588235 0.88627451\n",
      "  0.70980392 0.73333333 0.89411765 0.61568627 0.65490196 0.65490196\n",
      "  0.61960784 0.7372549  0.76862745 0.50196078 0.66666667 0.70196078\n",
      "  0.50196078 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.1372549  0.36078431 0.50588235\n",
      "  0.78039216 0.72156863 0.67058824 0.70196078 0.73333333 0.96470588\n",
      "  0.66666667 0.74117647 0.94117647 0.61176471 0.66666667 0.68235294\n",
      "  0.65098039 0.74509804 0.90196078 0.54509804 0.4        0.54509804\n",
      "  0.34901961 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.55686275 0.83529412 0.65098039 0.59215686 0.69803922 0.92156863\n",
      "  0.74509804 0.63529412 0.95686275 0.70196078 0.71764706 0.62745098\n",
      "  0.55686275 0.64313725 0.81568627 0.49019608 0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.00392157 0.00784314\n",
      "  0.         0.37647059 0.70980392 0.70196078 0.69803922 0.69803922\n",
      "  0.7254902  0.62745098 0.81568627 0.67058824 0.65490196 0.65490196\n",
      "  0.72156863 0.68235294 0.41176471 0.         0.00392157 0.00784314\n",
      "  0.00392157 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.00784314 0.         0.00392157 0.20392157 0.3254902  0.38431373\n",
      "  0.43921569 0.5254902  0.57254902 0.41176471 0.34901961 0.28627451\n",
      "  0.19215686 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 784) (12000, 784)\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.reshape(-1, 784)\n",
    "x_val = x_val.reshape(-1, 784)\n",
    "print(x_train.shape, x_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[0].reshape(28,28), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.utils.to_categorical([0, 1, 3]) # 원 핫 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 10) (12000, 10)\n"
     ]
    }
   ],
   "source": [
    "y_train_encoded = tf.keras.utils.to_categorical(y_train)\n",
    "y_val_encoded = tf.keras.utils.to_categorical(y_val)\n",
    "print(y_train_encoded.shape, y_val_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(y_train[0], y_train_encoded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........................................"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fc = MultiClassNetwork(units=100, batch_size=256) # x_train_batch.shape = (256,784) w1.shape=(784,100)\n",
    "                                                  # b1.shape = (100,)\n",
    "                                                  # a1.shape = (256,100) , w2.shape = (100,10)\n",
    "                                                  # b2.shape = (10,) , a2.shape(256,10)\n",
    "fc.fit(x_train, y_train_encoded, \n",
    "       x_val=x_val, y_val=y_val_encoded, epochs=40)\n",
    "plt.plot(fc.losses)\n",
    "plt.plot(fc.val_losses)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('iteration')\n",
    "plt.legend(['train_loss', 'val_loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 값 예측하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 784)\n",
      "예측값: 6\n",
      "셔츠\n",
      "실제값: 6\n",
      "셔츠\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAT4ElEQVR4nO3da2xWVboH8P9joVxquUMtUKFcBbk5oIKDB8E4ckmUkcSoyQSNGSZmJjqJRgnnw8iHE/XkjJ75cDJJR2DQjIwmI1EjggxRiaBoi9wvirUCpbSA5ZZSoO1zPnQz6Wj389T30v3K+v+Spu3+d/Vd3fRhv33XXmuJqoKIrn7XJN0BIuocLHaiQLDYiQLBYicKBIudKBBdOvPBRIQv/Xey/v37m3lBQYGZi4iZNzU1mXljY2NsdurUKbMtpUZV2/1HS6vYRWQugD8ByAPwsqo+n873u1pdc439BMorqObm5pQfe8GCBWY+Y8YMM+/Sxf4Vqa+vN/P9+/fHZqtWrTLberzzls1h5SQfO1UpP40XkTwA/wdgHoDxAB4UkfGZ6hgRZVY6f7PfAuCQqlaq6iUAfwdwb2a6RUSZlk6xDwFwpM3nR6Nj/0ZElohIuYiUp/FYRJSmrL9Ap6plAMoAvkBHlKR0ruzVAErafD40OkZEOSidYv8cwGgRKRWRfAAPAHg7M90iokyTdIYIRGQ+gP9F69DbSlX9L+fr+TQ+CyZNmhSb7dy502y7detWM29paTFzb5x95syZsVn37t3NtukMOQL28FguDo1lSlbG2VV1HYB16XwPIuocvF2WKBAsdqJAsNiJAsFiJwoEi50oECx2okCkNc7+ox+M4+ztuuGGG8y8qKjIzGtra2OzXr16mW2XL19u5l7706dPm/kbb7wRmx0+fNhsa43RA8ALL7xg5pcuXTLzq1XcODuv7ESBYLETBYLFThQIFjtRIFjsRIFgsRMFolOXkr5aTZ061cwXLlxo5sXFxWa+ZcsWM+/Tp09s5i3XfPDgQTMfNGiQmTc0NJi5NcU2Pz/fbHv27Fkzf/rpp838ww8/jM0OHDhgtj158qSZ/xTxyk4UCBY7USBY7ESBYLETBYLFThQIFjtRIFjsRIHgFNcOsqZTbtq0yWw7btw4M/fGdPfu3Wvmw4cPj83mz59vtq2oqDBzbwdabznowsLC2GzDhg1m2969e5v59OnTzTwvLy82O3/+vNl27dq1Zn7o0CEzTxKnuBIFjsVOFAgWO1EgWOxEgWCxEwWCxU4UCBY7USA4nz0yYcIEM7/nnntis2eeecZsW1VVZebetseVlZUpf/++ffuabVetWmXmI0aMMPMePXqY+ZQpU2Kzbdu2mW179uxp5seOHTPz6urqlPoFAE8++aSZP/bYY2aei9IqdhGpAnAOQDOAJlWdlolOEVHmZeLKPltVr75lPYiuMvybnSgQ6Ra7AnhfRCpEZEl7XyAiS0SkXETK03wsIkpDuk/jZ6pqtYgMArBRRA6o6ua2X6CqZQDKgJ/2RBiin7q0ruyqWh29rwOwFsAtmegUEWVeysUuIgUiUnjlYwC/ALAnUx0josxK52l8EYC1InLl+7ymqusz0qsETJtmjxrOnTs3NnvkkUfMtt668d58dm+N87Fjx8Zm1v0BgL8lszVXHvDXlR8zZkxsVldXl3JbABg5cqSZW2vm79u3z2z77rvvmvlPUcrFrqqVACZnsC9ElEUceiMKBIudKBAsdqJAsNiJAsFiJwoEp7hG5syZY+bffPNNbGZtSwz4Ww97yxrv2WPfvjBs2LDYrKamxmzrLYM9atQoM+/atauZT5w4MTY7ceKE2baoqMjMa2trzbxLl9R/vYcOHWrmAwYMMPNc3PKZV3aiQLDYiQLBYicKBIudKBAsdqJAsNiJAsFiJwoEx9kj3lTPkpKS2Ky83F5xyxvr7tatm5mfPn3azPv06RObectUe1sPe9smX7hwwcytaareOa+vrzfzxsZGM//oo49is0WLFpltvfsL+vfvb+YcZyeixLDYiQLBYicKBIudKBAsdqJAsNiJAsFiJwoEx9kj3li2tfXxvHnzzLbemKu37bE3b7u0tDQ285aC9vJx48aZubVcM2Bv+bxixQqz7eDBg8188mR7ceNZs2bFZrfddpvZtqGhwcy9eyNyEa/sRIFgsRMFgsVOFAgWO1EgWOxEgWCxEwWCxU4UCI6zRyoqKsx89erVsZk3ZmuNgwP+3Oji4mIzt+4BuPbaa8221lx4ACgsLDTzvLw8M7fWV/fWZh89erSZFxQUmPnAgQNjM28NAu++i++++87Mc5F7ZReRlSJSJyJ72hzrJyIbReSr6H38bxsR5YSOPI3/K4C53zu2FMAmVR0NYFP0ORHlMLfYVXUzgO8/Z7kXwJXntasBLMxst4go01L9m71IVa8srHYcQOymXCKyBMCSFB+HiDIk7RfoVFVFRI28DEAZAFhfR0TZlerQW62IFANA9L4uc10iomxItdjfBrA4+ngxgLcy0x0iyhb3abyIrAFwB4ABInIUwB8APA/gDRF5FMC3AO7PZiczYcKECWb+wAMPmPmaNWtiMxEx23p7mJ85c8bMvf3brfb5+flmWy/3+u6x5rt7Y9nemvfNzc1mfunSpdhs/fr1ZtvrrrvOzGfPnm3mr776qpknwS12VX0wJrozw30hoizi7bJEgWCxEwWCxU4UCBY7USBY7ESBCGaKqzfV0xtqefjhh2Oz+fPnm22XL19u5l9++aWZe0tJW8NjQ4YMMdt+8sknZu4Nb504ccLMramg3nbR3ve2pvYCwNq1a2Mzb4lsb5lqb0p0Lg698cpOFAgWO1EgWOxEgWCxEwWCxU4UCBY7USBY7ESBCGacfd++fWa+bNkyM3///fdjM288eNGiRWbuTXE9evSombe0tMRmDz30kNm2srLSzK0tlwF/W+Xbb789NquvrzfblpSUmLm3zLVq/MJI69atM9t+8MEHZu79PuUiXtmJAsFiJwoEi50oECx2okCw2IkCwWInCgSLnSgQwYyze9v/jhkzxsyted2DBg0y23rbGnt5jx49zNzqmzdWPX78eDP35n1369bNzK1ltr1lqq+//noz79evn5nv3bs3NvPWCPB+XyZNmmTmu3btMvMk8MpOFAgWO1EgWOxEgWCxEwWCxU4UCBY7USBY7ESB4Dh7pLGx0cytMeH777d3rF66dKmZW+PBgL+18TXXxP+f3dDQYLZ97bXXzPymm24yc++8lZaWxmbvvfee2dZb094bZ3/ppZdiM+/n6tmzp5lfvnzZzPv06WPm3r9pNrhXdhFZKSJ1IrKnzbFnRaRaRHZEb/YuCUSUuI48jf8rgLntHH9JVadEb/ayH0SUOLfYVXUzgPg9fIjoJyGdF+h+JyK7oqf5sZtuicgSESkXkfI0HouI0pRqsf8ZwEgAUwDUAPhj3BeqapmqTlPVaSk+FhFlQErFrqq1qtqsqi0A/gLglsx2i4gyLaViF5HiNp/+EsCeuK8lotzgjrOLyBoAdwAYICJHAfwBwB0iMgWAAqgC8JvsdTEzpk6daubWPuIA0L9//9hs7NixZtumpiYznz17tpl7+7dbe8/PmjXLbPvFF1+YuTfP3xtPts7b5s2bzbYzZsww80uXLpn54cOHYzNvnL26utrMBwwYkFaexDi7W+yq+mA7h1dkoS9ElEW8XZYoECx2okCw2IkCwWInCgSLnSgQwUxx3bp1q5lv27bNzCdMmBCbffzxx2Zbb2ti63sD/pLL1hRXaylnry3gT5EdOHBgyt+/Sxf718/7ub2ht/Pnz8dmvXr1Mtt6S0F77b1tvJPAKztRIFjsRIFgsRMFgsVOFAgWO1EgWOxEgWCxEwUimHF2b0rj119/beZTpkyJzbzpkMXFxWY+dOhQMz9+/LiZFxYWxmbetsfels7WUtCA33dru+mioiKzrdf3U6dOmbk1Ndgbw/f+Tb17BPr2jV2pDQBw5swZM88GXtmJAsFiJwoEi50oECx2okCw2IkCwWInCgSLnSgQwYyzL1iwwMy9ed9PPPFEbLZhwwazbUVFhZm3tLSY+fbt283cGiv/7LPPzLbedtHefHdvvNgaj965c6fZ1hur9tYJGDRoUGz24osvmm295cGHDBli5s8995yZV1VVmXk28MpOFAgWO1EgWOxEgWCxEwWCxU4UCBY7USBY7ESBCGac/amnnjLzTz/91MytbZG9ufDetsbe3OjGxkYzt7b/9ebCe/O2vXF2b65+7969YzNVNdseOXLEzLt3727m+fn5sdnLL79stvX2AvDOi9c+Ce6VXURKROQDEdknIntF5InoeD8R2SgiX0Xv7TsgiChRHXka3wTgSVUdD2A6gN+KyHgASwFsUtXRADZFnxNRjnKLXVVrVHV79PE5APsBDAFwL4DV0ZetBrAwS30kogz4UX+zi8hwADcB2AagSFVroug4gHYXFBORJQCWpNFHIsqADr8aLyLXAvgHgN+r6tm2mba+0tLuqy2qWqaq01R1Wlo9JaK0dKjYRaQrWgv9b6r6ZnS4VkSKo7wYQF12ukhEmeA+jZfWuZ8rAOxX1bbzAt8GsBjA89H7t7LSwwwZOXKkmV+8eNHM8/LyYrODBw+abe+8804zv++++8x86tSpZj548ODYbPHixWZbb1jQW8553LhxZm4Nj3nDdt7y3/369TPzjRs3xmbeVtPeMtfWsB5gDzkCyWzp3JG/2X8O4FcAdovIjujYMrQW+Rsi8iiAbwHcn5UeElFGuMWuqh8DiFvZwb5kEVHO4O2yRIFgsRMFgsVOFAgWO1EgWOxEgQhmimtBQYGZe+OuVl5eXm629ZaCtrYWBoAtW7aY+aRJk2KzCxcumG1ff/11M7/xxhvN3PvZrKmga9asMdt6S3B74+zr16+Pzbyfy/t9saY8A0DPnj3NPAm8shMFgsVOFAgWO1EgWOxEgWCxEwWCxU4UCBY7USCCGWcvLCw086FDh5r5qFGjYrOGhgaz7d13323m1lx5wF9q2poXvn//frOtt5yz97Pt2rXLzK11BKwlsAGgrs5eD8Wbc26dl3Pnzplthw0bZubeOLu3zHUSeGUnCgSLnSgQLHaiQLDYiQLBYicKBIudKBAsdqJABDPOvnv3bjP3tmweO3ZsbHb58mWzrTfG77X31iCfPn16bHby5Emz7V133WXm3rzuyspKM7/11ltjM2tdd8C/92H48OFmbq0TsHnzZrPt+PHjzfzs2bNm7m3jnQRe2YkCwWInCgSLnSgQLHaiQLDYiQLBYicKBIudKBAd2Z+9BMArAIoAKIAyVf2TiDwL4NcArmw0vUxV12Wro+n69ttvzXzOnDlmbu1T3tLSYradPHmymR87dszMvTXIS0tLY7P6+nqzbXNzs5l37drVzL2+WfO6vfsPvDH+kpISMxeJ23wYuHjxotnWmytfXV1t5t55T0JHbqppAvCkqm4XkUIAFSJy5W6Il1T1f7LXPSLKlI7sz14DoCb6+JyI7AcwJNsdI6LM+lF/s4vIcAA3AdgWHfqdiOwSkZUi0jemzRIRKRcRe48kIsqqDhe7iFwL4B8Afq+qZwH8GcBIAFPQeuX/Y3vtVLVMVaep6rT0u0tEqepQsYtIV7QW+t9U9U0AUNVaVW1W1RYAfwFwS/a6SUTpcotdWl/SXAFgv6q+2OZ426U7fwlgT+a7R0SZ0pFX438O4FcAdovIjujYMgAPisgUtA7HVQH4TRb6lzF79tj/Fz3++ONmfvPNN6f82K+88oqZW1NUAX94zFrW+NSpU2bbESNGmHlTU5OZe0Nv1vCZN2SZn59v5t7w1oEDB2Iza5trAJg4caKZV1VVmbm3RHcSOvJq/McA2huwzNkxdSL6Id5BRxQIFjtRIFjsRIFgsRMFgsVOFAgWO1EggllK2hsvfvPNN828pqYm5cf2xvi93LNy5crYrKKiwmw7b948M/emcnrjzdZ527dvX1rf+5133jFzi3devHsAjhw5Yua5OM7OKztRIFjsRIFgsRMFgsVOFAgWO1EgWOxEgWCxEwVCOnM8UEROAGi7pvMAAPaewsnJ1b7lar8A9i1VmezbMFUd2F7QqcX+gwcXKc/VtelytW+52i+AfUtVZ/WNT+OJAsFiJwpE0sVelvDjW3K1b7naL4B9S1Wn9C3Rv9mJqPMkfWUnok7CYicKRCLFLiJzReSgiBwSkaVJ9CGOiFSJyG4R2ZH0/nTRHnp1IrKnzbF+IrJRRL6K3re7x15CfXtWRKqjc7dDROYn1LcSEflARPaJyF4ReSI6nui5M/rVKeet0/9mF5E8AF8CuAvAUQCfA3hQVe2VDDqJiFQBmKaqid+AISL/AeA8gFdUdUJ07L8BfKeqz0f/UfZV1WdypG/PAjif9Dbe0W5FxW23GQewEMDDSPDcGf26H51w3pK4st8C4JCqVqrqJQB/B3BvAv3Ieaq6GcB33zt8L4DV0cer0frL0uli+pYTVLVGVbdHH58DcGWb8UTPndGvTpFEsQ8B0HZNn6PIrf3eFcD7IlIhIkuS7kw7ilT1ylpPxwEUJdmZdrjbeHem720znjPnLpXtz9PFF+h+aKaq/gzAPAC/jZ6u5iRt/Rssl8ZOO7SNd2dpZ5vxf0ny3KW6/Xm6kij2agAlbT4fGh3LCapaHb2vA7AWubcVde2VHXSj93UJ9+dfcmkb7/a2GUcOnLsktz9Potg/BzBaREpFJB/AAwDeTqAfPyAiBdELJxCRAgC/QO5tRf02gMXRx4sBvJVgX/5NrmzjHbfNOBI+d4lvf66qnf4GYD5aX5H/GsB/JtGHmH6NALAzetubdN8ArEHr07rLaH1t41EA/QFsAvAVgH8C6JdDfXsVwG4Au9BaWMUJ9W0mWp+i7wKwI3qbn/S5M/rVKeeNt8sSBYIv0BEFgsVOFAgWO1EgWOxEgWCxEwWCxU4UCBY7USD+H6IwMptJf8mXAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 7\n",
    "x = x_test[index:index+1].reshape(1,784)\n",
    "print(x.shape)\n",
    "x = x/255.\n",
    "x\n",
    "y_pred = fc.predict(x)\n",
    "print(\"예측값:\", y_pred[0])\n",
    "print(class_names[y_pred[0]])\n",
    "print(\"실제값:\", y_test[index])\n",
    "print(class_names[y_test[index]])\n",
    "plt.imshow(x_test[index], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7997\n"
     ]
    }
   ],
   "source": [
    "x_scaled = x_test/255.\n",
    "y_test_encoded = tf.keras.utils.to_categorical(y_test)\n",
    "x = x_scaled.reshape(-1,784)\n",
    "score = fc.score(x, y_test_encoded)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다중 분류 케라스로 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "1500/1500 [==============================] - 4s 2ms/step - loss: 1.3708 - accuracy: 0.6454 - val_loss: 0.9568 - val_accuracy: 0.7283\n",
      "Epoch 2/40\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.8439 - accuracy: 0.7379 - val_loss: 0.7539 - val_accuracy: 0.7549\n",
      "Epoch 3/40\n",
      "1500/1500 [==============================] - 4s 2ms/step - loss: 0.7164 - accuracy: 0.7612 - val_loss: 0.6687 - val_accuracy: 0.7738\n",
      "Epoch 4/40\n",
      "1500/1500 [==============================] - 4s 2ms/step - loss: 0.6515 - accuracy: 0.7763 - val_loss: 0.6168 - val_accuracy: 0.7908\n",
      "Epoch 5/40\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.6092 - accuracy: 0.7907 - val_loss: 0.5814 - val_accuracy: 0.8006\n",
      "Epoch 6/40\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.5790 - accuracy: 0.8002 - val_loss: 0.5542 - val_accuracy: 0.8084\n",
      "Epoch 7/40\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.5556 - accuracy: 0.8078 - val_loss: 0.5338 - val_accuracy: 0.8133\n",
      "Epoch 8/40\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.5371 - accuracy: 0.8149 - val_loss: 0.5168 - val_accuracy: 0.8207\n",
      "Epoch 9/40\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.5222 - accuracy: 0.8200 - val_loss: 0.5032 - val_accuracy: 0.8241\n",
      "Epoch 10/40\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.5101 - accuracy: 0.8238 - val_loss: 0.4927 - val_accuracy: 0.8297\n",
      "Epoch 11/40\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4996 - accuracy: 0.8269 - val_loss: 0.4825 - val_accuracy: 0.8312\n",
      "Epoch 12/40\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4903 - accuracy: 0.8297 - val_loss: 0.4765 - val_accuracy: 0.8323\n",
      "Epoch 13/40\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4824 - accuracy: 0.8321 - val_loss: 0.4671 - val_accuracy: 0.8352\n",
      "Epoch 14/40\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4753 - accuracy: 0.8342 - val_loss: 0.4611 - val_accuracy: 0.8381\n",
      "Epoch 15/40\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4689 - accuracy: 0.8354 - val_loss: 0.4564 - val_accuracy: 0.8386\n",
      "Epoch 16/40\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4635 - accuracy: 0.8374 - val_loss: 0.4488 - val_accuracy: 0.8421\n",
      "Epoch 17/40\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4580 - accuracy: 0.8398 - val_loss: 0.4457 - val_accuracy: 0.8432\n",
      "Epoch 18/40\n",
      "1500/1500 [==============================] - 4s 2ms/step - loss: 0.4530 - accuracy: 0.8408 - val_loss: 0.4412 - val_accuracy: 0.8453\n",
      "Epoch 19/40\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4489 - accuracy: 0.8429 - val_loss: 0.4370 - val_accuracy: 0.8466\n",
      "Epoch 20/40\n",
      "1500/1500 [==============================] - 4s 2ms/step - loss: 0.4442 - accuracy: 0.8443 - val_loss: 0.4344 - val_accuracy: 0.8454\n",
      "Epoch 21/40\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4406 - accuracy: 0.8456 - val_loss: 0.4304 - val_accuracy: 0.8462\n",
      "Epoch 22/40\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4367 - accuracy: 0.8463 - val_loss: 0.4270 - val_accuracy: 0.8490\n",
      "Epoch 23/40\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 0.4334 - accuracy: 0.8484 - val_loss: 0.4231 - val_accuracy: 0.8506\n",
      "Epoch 24/40\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4301 - accuracy: 0.8493 - val_loss: 0.4208 - val_accuracy: 0.8514\n",
      "Epoch 25/40\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4271 - accuracy: 0.8510 - val_loss: 0.4172 - val_accuracy: 0.8536\n",
      "Epoch 26/40\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4239 - accuracy: 0.8500 - val_loss: 0.4167 - val_accuracy: 0.8554\n",
      "Epoch 27/40\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4212 - accuracy: 0.8519 - val_loss: 0.4132 - val_accuracy: 0.8528\n",
      "Epoch 28/40\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4187 - accuracy: 0.8528 - val_loss: 0.4104 - val_accuracy: 0.8543\n",
      "Epoch 29/40\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4159 - accuracy: 0.8532 - val_loss: 0.4078 - val_accuracy: 0.8569\n",
      "Epoch 30/40\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4135 - accuracy: 0.8539 - val_loss: 0.4061 - val_accuracy: 0.8572\n",
      "Epoch 31/40\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4110 - accuracy: 0.8550 - val_loss: 0.4047 - val_accuracy: 0.8587\n",
      "Epoch 32/40\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4084 - accuracy: 0.8558 - val_loss: 0.4114 - val_accuracy: 0.8524\n",
      "Epoch 33/40\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4065 - accuracy: 0.8578 - val_loss: 0.4000 - val_accuracy: 0.8598\n",
      "Epoch 34/40\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4044 - accuracy: 0.8566 - val_loss: 0.3984 - val_accuracy: 0.8595\n",
      "Epoch 35/40\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4023 - accuracy: 0.8579 - val_loss: 0.3981 - val_accuracy: 0.8612\n",
      "Epoch 36/40\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4004 - accuracy: 0.8579 - val_loss: 0.3977 - val_accuracy: 0.8591\n",
      "Epoch 37/40\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3981 - accuracy: 0.8597 - val_loss: 0.3932 - val_accuracy: 0.8618\n",
      "Epoch 38/40\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3964 - accuracy: 0.8599 - val_loss: 0.3938 - val_accuracy: 0.8612\n",
      "Epoch 39/40\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3942 - accuracy: 0.8609 - val_loss: 0.3922 - val_accuracy: 0.8598\n",
      "Epoch 40/40\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3926 - accuracy: 0.8613 - val_loss: 0.3894 - val_accuracy: 0.8648\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "model = Sequential()\n",
    "model.add(Dense(100, activation='sigmoid', input_shape=(784,)))  # w1.shape = (784,100), b1.shape=(100,)\n",
    "model.add(Dense(10, activation='softmax'))           # w2.shape=(100,10), b2.shape=(10,)\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(x_train, y_train_encoded, epochs=40, \n",
    "                    validation_data=(x_val, y_val_encoded))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_loss', 'val_loss'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_accuracy', 'val_accuracy'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(x_val, y_val_encoded, verbose=0)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 합성곱 신경망(CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.array([2, 8, 3, 7, 1, 2, 0, 4, 5])\n",
    "w = np.array([2, 1, 5, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_r = w[::-1]\n",
    "print(w_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_r = np.flip(w)\n",
    "print(w_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(x)\n",
    "F = len(w_r)\n",
    "O = N - F + 1\n",
    "for i in range(O):\n",
    "#     print(np.dot(x[i:i+4], w_r.reshape(-1,1)), end=' ')  # (4,)(4,1) => (1,)\n",
    "#   print(np.dot(x[i:i+4], w_r), end=' ')  # (4,)(4,) => () 스칼라\n",
    "   print(np.sum(x[i:i+4]*w_r), end=' ')  # (4,)(4,) => () 스칼라\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import convolve\n",
    "convolve(x, w, mode='valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import correlate\n",
    "correlate(x, w, mode='valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import correlate\n",
    "correlate(x, w, mode='full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import correlate\n",
    "correlate(x, w, mode='same')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.array([[1, 2, 3],\n",
    "              [4, 5, 6],\n",
    "              [7, 8, 9]])\n",
    "w = np.array([[2, 0], [0, 0]])\n",
    "from scipy.signal import correlate2d\n",
    "correlate2d(x, w, mode='valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[1, 2, 3],\n",
    "              [4, 5, 6],\n",
    "              [7, 8, 9]])\n",
    "w = np.array([[2, 0], [0, 0]])\n",
    "from scipy.signal import correlate2d\n",
    "correlate2d(x, w, mode='same')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(7*7).reshape(7,7)\n",
    "print(x)\n",
    "w = np.ones((3,3))\n",
    "print(w)\n",
    "from scipy.signal import correlate2d\n",
    "correlate2d(x, w, mode='valid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 렐루 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "# x = np.array([-1,3,4,5,-2,3,4,5])\n",
    "# y = relu(x)\n",
    "# print(y)\n",
    "\n",
    "x = np.arange(-5, 5, 0.01)\n",
    "y = relu(x)\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 합성곱 연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import *\n",
    "import matplotlib.pyplot as plt\n",
    "image = tf.constant([[[[1],[2],[3]],\n",
    "                      [[4],[5],[6]],\n",
    "                      [[7],[8],[9]]]], dtype=np.float32)\n",
    "print(image.shape)\n",
    "print(image)\n",
    "print(image.numpy())\n",
    "print(image.numpy().reshape(3,3).shape)\n",
    "print(image.numpy().reshape(3,3))\n",
    "plt.imshow(image.numpy().reshape(3,3), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = np.array([[[[1.]], [[1.]]],[[[1.]],[[1.]]]])\n",
    "print(\"weight.shape=\", weight.shape) # (height, width, channel, FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weight_init = tf.constant_initializer(weight)\n",
    "# print(type(weight_init))\n",
    "conv2d = tf.keras.layers.Conv2D(filters=1, kernel_size=2, padding='valid', kernel_initializer=weight_init)(image)\n",
    "print(\"conv2d.shape\", conv2d.shape) # N-F+1=O  (1,2,2,1)\n",
    "print(conv2d.numpy())\n",
    "print(conv2d.numpy().reshape(2,2))\n",
    "plt.imshow(conv2d.numpy().reshape(2,2), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_init = tf.constant_initializer(weight)\n",
    "# print(type(weight_init))\n",
    "conv2d = tf.keras.layers.Conv2D(filters=1, kernel_size=2, padding='same', kernel_initializer=weight_init)(image)\n",
    "print(\"conv2d.shape\", conv2d.shape) # N-F+1=O  (1,2,2,1)\n",
    "print(conv2d.numpy())\n",
    "print(conv2d.numpy().reshape(3,3))\n",
    "plt.imshow(conv2d.numpy().reshape(3,3), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = tf.constant([[[[1],[2],[3]],\n",
    "                      [[4],[5],[6]],\n",
    "                      [[7],[8],[9]]]], dtype=np.float32)\n",
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = np.array([[[[1.,10.,-1.]],[[1.,10.,-1.]]],[[[1.,10.,-1.]],[[1.,10.,-1.]]]])\n",
    "print(weight.shape)  # (height, width, channel, FN)\n",
    "print(weight)\n",
    "temp=np.transpose(weight,(3,0,1,2))\n",
    "for t in temp:\n",
    "    print(t.reshape(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_init = tf.constant_initializer(weight)\n",
    "conv2d = tf.keras.layers.Conv2D(filters=3, kernel_size=2, padding='valid', kernel_initializer=weight_init)(image)\n",
    "print(\"conv2d.shape\", conv2d.shape)  # (1,2,2,3)\n",
    "print(conv2d)\n",
    "\n",
    "feature_maps = np.swapaxes(conv2d, 0, 3)\n",
    "print(feature_maps.shape)\n",
    "\n",
    "for i, feature_map in enumerate(feature_maps):\n",
    "    print(feature_map.reshape(2,2))\n",
    "    plt.subplot(1,3,i+1), plt.imshow(feature_map.reshape(2,2), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weight_init = tf.constant_initializer(weight)\n",
    "conv2d = tf.keras.layers.Conv2D(filters=3, kernel_size=2, padding='same', kernel_initializer=weight_init)(image)\n",
    "print(\"conv2d.shape\", conv2d.shape)\n",
    "# print(conv2d)\n",
    "\n",
    "feature_maps = np.swapaxes(conv2d, 0, 3)\n",
    "for i, feature_map in enumerate(feature_maps):\n",
    "    print(feature_map.reshape(3,3))\n",
    "#     plt.subplot(1,3,i+1), plt.imshow(feature_map.reshape(3,3), cmap='gray')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = tf.constant(  [[ \n",
    "                         [[1,0,1],[1,1,1],[1,1,1],[0,0,1],[0,1,0]], \n",
    "                         [[0,0,1],[1,1,1],[1,1,1],[1,1,1],[0,0,0]], \n",
    "                         [[0,0,0],[0,0,0],[1,1,0],[1,1,1],[1,0,1]], \n",
    "                         [[0,0,0],[0,0,1],[1,1,1],[1,1,1],[0,1,0]], \n",
    "                         [[0,1,0],[1,1,1],[1,1,1],[0,0,0],[0,0,0]] \n",
    "                      ]],     dtype=np.float32)\n",
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maps = np.swapaxes(image, 0, 3)\n",
    "print(maps.shape)\n",
    "for i, map in enumerate(maps):\n",
    "    print(map.reshape(5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = np.array( [ \n",
    "                     [[[1],[0],[-1]], [[0],[-1],[0]], [[1],[0],[0]]],\n",
    "                     [[[0],[-1],[0]], [[1],[1],[1]], [[0],[-1],[0]]],\n",
    "                     [[[1],[1],[0]], [[0],[-1],[0]], [[1],[0],[-1]]]\n",
    "                   ] )\n",
    "\n",
    "print(weight.shape)\n",
    "# maps = np.swapaxes(weight, 1, 2)\n",
    "# maps = np.swapaxes(maps, 0, 1)\n",
    "\n",
    "maps = np.transpose(weight,(2,0,1,3))\n",
    "\n",
    "for i, map in enumerate(maps):\n",
    "    print(map.reshape(3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_init = tf.constant_initializer(weight)\n",
    "conv2d = tf.keras.layers.Conv2D(filters=1, kernel_size=3, padding='valid', kernel_initializer=weight_init)(image)\n",
    "print(\"conv2d.shape\", conv2d.shape)\n",
    "feature_maps = np.swapaxes(conv2d, 0, 3)\n",
    "for i, feature_map in enumerate(feature_maps):\n",
    "    print(feature_map.reshape(3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = np.array( [ \n",
    "                     [[[1,1],[0,1],[-1,-1]], [[0,0],[-1,0],[0,0]], [[1,1],[0,1],[0,0]]],\n",
    "                     [[[0,0],[-1,0],[0,0]], [[1,1],[1,1],[1,1]], [[0,0],[-1,0],[0,0]]],\n",
    "                     [[[1,1],[1,1],[0,0]], [[0,0],[-1,0],[0,0]], [[1,1],[0,1],[-1,-1]]]\n",
    "                   ] )\n",
    "\n",
    "print(weight.shape)  # (3,3,3,2)\n",
    "maps = np.swapaxes(weight, 1, 2)\n",
    "maps = np.swapaxes(maps, 0, 1)\n",
    "\n",
    "for map in maps:\n",
    "    map = np.swapaxes(map, 1, 2)\n",
    "    map = np.swapaxes(map, 0, 1)\n",
    "    for filter in map:\n",
    "        print(filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (3,3,3,2) => (3,2,3,3)\n",
    "weight = np.array( [ \n",
    "                     [[[1,1],[0,1],[-1,-1]], [[0,0],[-1,0],[0,0]], [[1,1],[0,1],[0,0]]],\n",
    "                     [[[0,0],[-1,0],[0,0]], [[1,1],[1,1],[1,1]], [[0,0],[-1,0],[0,0]]],\n",
    "                     [[[1,1],[1,1],[0,0]], [[0,0],[-1,0],[0,0]], [[1,1],[0,1],[-1,-1]]]\n",
    "                   ] )\n",
    "\n",
    "maps = np.transpose(weight, (2,3,0,1) )\n",
    "\n",
    "for map in maps:\n",
    "    for filter in map:\n",
    "        print(filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weight_init = tf.constant_initializer(weight)\n",
    "conv2d = tf.keras.layers.Conv2D(filters=2, kernel_size=3, padding='valid', kernel_initializer=weight_init)(image)\n",
    "print(\"conv2d.shape\", conv2d.shape)  # (1,3,3,2)\n",
    "feature_maps = np.swapaxes(conv2d, 0, 3)\n",
    "for feature_map in feature_maps:\n",
    "    print(feature_map.reshape(3,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 고차원 텐서 연습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(12).reshape(3,2,2)\n",
    "b = np.swapaxes(a, 0, 1)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(12).reshape(3,2,2)\n",
    "b = np.swapaxes(a, 1, 2)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(12).reshape(3,2,2)\n",
    "b = np.swapaxes(a, 0, 2)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = np.arange(24).reshape(2,3,2,2)\n",
    "b = np.swapaxes(a, 0, 2) \n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(24).reshape  (2,3,2,2)\n",
    "b = np.swapaxes(a, 1, 3) \n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = tf.constant([ [ [ [4], [3] ], [[2],[1]] ] ], dtype=np.float32)\n",
    "print(image.shape)\n",
    "pool = tf.keras.layers.MaxPool2D(pool_size=(2,2), strides=1, padding='valid')(image)\n",
    "print(pool.shape)\n",
    "print(pool.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 맥스 풀링 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = tf.constant([[[[4],[3]],[[2],[1]]]], dtype=np.float32)\n",
    "pool = tf.keras.layers.MaxPool2D(pool_size=(2,2), strides=1, padding='valid')(image)\n",
    "print(pool.shape)\n",
    "print(pool.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = tf.constant([[[[4],[3]],[[2],[1]]]], dtype=np.float32)\n",
    "pool = tf.keras.layers.MaxPool2D(pool_size=(2,2), strides=1, padding='same')(image)\n",
    "print(pool.shape)\n",
    "print(pool.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = tf.constant([[[[0],[1],[2],[3]],\n",
    "                      [[4],[5],[6],[7]],\n",
    "                      [[8],[9],[10],[11]],\n",
    "                      [[12],[13],[14],[15]]]], dtype=np.float32)\n",
    "print(image.shape)\n",
    "pool = tf.keras.layers.MaxPool2D(pool_size=(2,2), strides=2, padding='valid')(image)\n",
    "print(pool.shape)\n",
    "print(pool.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "class_names = ['0','1','2','3','4','5','6','7','8','9']\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "train_images = train_images.astype(np.float32) / 255.\n",
    "test_images = test_images.astype(np.float32) / 255.\n",
    "\n",
    "print(train_images.shape)\n",
    "img = train_images[0]\n",
    "plt.imshow( img, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = train_images[0]\n",
    "img = img.reshape(-1,28,28,1)\n",
    "img = tf.convert_to_tensor(img)\n",
    "\n",
    "weight_init = tf.keras.initializers.RandomNormal(stddev=0.01)\n",
    "conv2d = tf.keras.layers.Conv2D(filters=5, kernel_size=3, padding='same' \n",
    "                                , kernel_initializer=weight_init)(img)\n",
    "print(\"conv2d.shape\", conv2d.shape)  # (1,28,28,5)\n",
    "feature_maps = np.swapaxes(conv2d, 0, 3)\n",
    "for i, feature_map in enumerate(feature_maps):\n",
    "    plt.subplot(1,5,i+1), plt.imshow(feature_map.reshape(28,28), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = tf.keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2), padding='valid')(conv2d)\n",
    "print(pool.shape)\n",
    "feature_maps = np.swapaxes(pool, 0, 3)\n",
    "for i, feature_map in enumerate(feature_maps):\n",
    "    plt.subplot(1,5,i+1), plt.imshow(feature_map.reshape(14,14), cmap='gray')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
